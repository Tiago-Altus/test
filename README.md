**Services Portal API Endpoi**** nts**([https://github.com/guymontreuil/services-portal](https://github.com/guymontreuil/services-portal))

- Routes
  - Flow
    - Update Terms and Conditions
    - Copy file from SharePoint to S3
    - Create file from string
    - Copy to SharePoint
    - Create folder in SharePoint
  - Main
    - Check if Super Admin
    - Get Project Contacts
    - Get Tableau View Image
    - Copy file to S3 bucket
    - Copy folder to S3 bucket
    - Tableau Document Generation Helper
- Controllers
  - js
  - js
- Middleware
  - Error-handler
  - Is-auth
- Models
- Util
  - Common
  - Copy to s3 helper
  - Database
  - Error-Management
  - Get-tableau-image-helper
  - Logger
  - Update-t-and-c-helper

**Client Access Portal Terms and Conditions**

The purpose of this project is to allow the legal team to have an easy way to maintain the Terms and Conditions documents for the Client Access Portal. The endpoint of our API is designed to take some request parameters.

**Note** :  This endpoint depends on a program called[pdf2htmlEX](https://github.com/coolwanglu/pdf2htmlEX).  There are installation instructions[here](https://github.com/coolwanglu/pdf2htmlEX/wiki/Download), for a variety of operating systems.  There is also an optional environment variable being used by this endpoint if it is specified.  This environment variable, _PDF2HTML\_DATA\_DIR_, is intended to be the location of the data directory used by pdf2htmlEX (i.e. _/path/to/pdf2htmlEX/data_).  We found that this environment variable must be specified on Windows machines only, as it seems that only Windows machines require the location of the data directory in order to use the pdf2htmlEX program.

**Update Terms and Conditions**

Pull the specified docx file from SharePoint, convert it to html, and add information to database for future reference.

POST  https://{serverip}/api/flow/updateTermsAndConditions

Headers

Content-Type application/json

Authorization \&lt;token\&gt;

Body

 {

    &quot;filename&quot;: &quot;\&lt;T and C.docx\&gt;&quot;,

    &quot;filepath&quot;: &quot;Shared Documents/TermsAndConditions/\&lt;Application Name\&gt;&quot;

}

Response

Success - HTTP 200

{

    &quot;message&quot;: &quot;Terms and Conditions [Created|Updated]&quot;,

     &quot;actualStatusCode&quot;: 200

}

This API request is generated by Microsoft Flow when a file is added to an application folder in SharePoint, which means that the legal teams never have to interact with the API directly. They simply drop a new Terms and Conditions docx document into the correct application folder on SharePoint and after about a minute, Microsoft Flow will send out an email informing that the Terms and Conditions document has been successfully updated or that there was an error updating the Terms and Conditions document.

**Check if Super Admin**

Given a token encrypting an email address, checks if the email corresponds to a user who is a Super Admin.

GET api/main/isSuperAdmin

Header

{

&quot;Content-Type: application/json&quot;

&quot;x-access-token&quot;: &quot;encryptedEmailToken&quot;

&quot;x-main-api-key:  mainapikey&quot;

}

HTTP Responses

Success - HTTP 200

{

&quot;superAdmin&quot;: true

}

**Data Transfer and Permissions Application to Files in the Client Access Portal S3 Bucket**

In order to help with establishing the Client Access Portal we had to copy around 1400 files from the Services Portal uploads folder and move them to an AWS S3 bucket. We created 3 API endpoints to assist with this task, which handles certain permissions and metadata had to be applied to folders so that the portal users only see the projects that they are allowed to see.

Note that in order to run these endpoints, the initS3.sh shell script must first be executed, which mounts the S3 bucket in the directory specified within the script.  In order to unmount the bucket, the shutdownS3.sh shell script can be run.  Both of these shell scripts can be found in the root directory of the GitHub repository.

### **Copy file to S3 bucket**

copyFileToS3, was intended to move one file at a time. G iven a file name, copies it from the Services Portal to an S3 bucket.

PUT http://{serverip}/api/main/copyFileToS3

Headers

{

    &quot;Content-Type&quot;: &quot;application/json&quot;,

    &quot;x-access-token&quot;: &quot;\&lt;token\&gt;&quot;

}

Body

 {

    &quot;filename&quot;: &quot;\&lt;filename\&gt;&quot;

}

HTTP Responses

Success - HTTP 200

{

    &quot;message&quot;: &quot;Successfully copied file \&lt;filename\&gt;&quot;

}

**Copy Folder to S3 bucket **

One endpoint, copyFolderToS3, was established in order to move all the files in the uploads folder at once.

PUT http://{serverip}/api/main/copyFolderToS3

Headers

{

&quot;Content-Type&quot;: &quot;application/json&quot;

&quot;X-access-token&quot; :  &quot;token&quot;

}

Responses

_Success - HTTP 200_

{

    &quot;message&quot;: &quot;Successfully copied \&lt;x\&gt; / \&lt;y\&gt; files&quot;

}

The final endpoint, copyFileSpToS3, was designed to move a single file from a SharePoint folder to an S3 bucket.

PUT http://{serverip}/api/main/copyFileSpToS3

Headers

Content-Type application/json

x-access-token  \&lt;token\&gt;

Body

 {

    &quot;filename&quot;: &quot;\&lt;filename\&gt;&quot;

}

These were designed for a single use situation but can be fairly easily be repurposed in order to allow for consistent updating or moving files from SharePoint.  For example, the copyFileSpToS3 endpoint is a Flow endpoint, so it would be easy to create a Flow which copies a file from a SharePoint folder into an S3 bucket.

** **

**Copy file from SharePoint to S3**

Moves a file from SharePoint to S3 bucket and initializes the permissions for the Client Access Portal.

**PUT api/flow/copyFileSpToS3**

Request header

{

    &quot;x-flow-api-key&quot;: &quot;\&lt;api key\&gt;&quot;,

    &quot;Content-Type&quot;: &quot;application/json&quot;

}

Request Body

{

             &quot;filename&quot;: &quot;filename&quot;

}

**HTTP Responses**

_Success - HTTP 200_

{

    &quot;message&quot;: &quot;Successfully copied file \&lt;filename\&gt;&quot;,

    &quot;actualStatusCode&quot;: 200

**Get Project Contacts**

Retrieves a list of project contacts from database.

**GET api/main/getProjectContacts**

Request header

{

    &quot;x-main-api-key&quot;: &quot;\&lt;api key\&gt;&quot;

}

\*\*HTTP Responses\*\*

\*Success - HTTP 200\*

```json

{

    &quot;message&quot;: &quot;Successfully retrieved project contacts list&quot;,

    &quot;contactsList&quot;: [

        {

                &quot;email&quot;: &quot;Contact1@altusgroup.com&quot;,

                &quot;name&quot;: &quot;Contact1&quot;

        },

        {

                &quot;email&quot;: &quot;Contact2@altusgroup.com&quot;,

                &quot;name&quot;: &quot;Contact2&quot;

        }

    ]

}

Tableau Document Generation Helper

The document generation helper is able to interact with Tableau in order to obtain png images of the specified view in Tableau. Moving forward, the images retrieved are meant to be formatted and used in automatic document generation. More information will be needed as to how exactly this will be implemented regarding storing the images once they are retrieved, and how the initial request might be made.

POST http://{serverip}/api/main/tableauDocGen

Headers

Content-Type application/json

Body

{

                &quot;tableauUser&quot;: &quot;\&lt;Tableau username\&gt;&quot;,

                &quot;tableauPassword&quot;: &quot;\&lt;encrypted Tableau password\&gt;&quot;,

                &quot;tableauViewName&quot;: &quot;\&lt;name of Tableau View\&gt;&quot;,

                &quot;tableauFilters \&lt;optional\&gt;&quot;: {

&quot;filter name 1&quot;: &quot;filter value 1&quot;,

&quot;filter name 2&quot;: &quot;filter value 2&quot;,

...

}

}

**Get Tableau View Image**

Retrieves an image of the specified view from a Tableau server.

**POST api/main/getTableauImage**

Request header

{

    &quot;Content-Type&quot;: &quot;application/json&quot;,

    &quot;x-main-api-key&quot;: &quot;\&lt;api key\&gt;&quot;

}

Request body

{

    &quot;tableauServer&quot;: &quot;tableauServer&quot;,

    &quot;tableauViewName&quot;: &quot;tableauView&quot;,

    &quot;tableauSite&quot;: &quot;tableauSite&quot;,

    &quot;tableauFilters&quot;: {

        &quot;filterName1&quot;: &quot;filterValue1&quot;,

        &quot;filterName2&quot;: &quot;filterValue2&quot;

    }

}

**HTTP Responses**

_Success - HTTP 200_

{

    &quot;message&quot;: &quot;Successfully retrieved Tableau View image&quot;,

    &quot;base64&quot;: &quot;image base64&quot;,

    &quot;width&quot;: 100,

    &quot;height&quot;: 100

}

**Altus SharePoint API Endpoints** **(see**[**https://github.com/guymontreuil/altus-sharepoint-api**](https://github.com/guymontreuil/altus-sharepoint-api)**)**

Create file from string is an endpoint that creates a file in the specified sharepoint folder with the value of a string that was passed in. this endpoint is used by estate master document generation program to create jsons in specific folders with flows that are triggered by them.

POST  https://{serverip}/api/flow/createFileFromString

Headers

Content-Type application/json

x-flow-api-key : \&lt;the x-flow-api-key value\&gt;

Body

 {

   &quot;fileName&quot;: &quot;\&lt;T and C.docx\&gt;&quot;,

    &quot;url&quot;:&quot;[https://altusgroup.sharepoint.com](https://altusgroup.sharepoint.com/)&quot;,

    &quot;site&quot;:&quot;TermsAndConditions&quot;,

    &quot;targetFolder&quot;:&quot;this/is/where/i/want/newfile&quot;,

    &quot;fileContents&quot;:&quot;\&lt;the contents of the file \&gt;&quot;,

    &quot;overwriteFile&quot;:&quot;\&lt;yes\&gt;or\&lt;no\&gt;&quot;

}



This endpoint is an endpoint that is meant to help copy files from any local file system to a specified sharepoint file system. As a bonus this endpoint should work on a s3 bucket mounted file system to move items in an s3 bucket to sharepoint

POST  https://{serverip}/api/flow/copyToSP

Headers

Content-Type application/json

x-flow-api-key : \&lt;the x-flow-api-key value\&gt;

Body

 {

    &quot;fileName&quot;: &quot;\&lt;T and C.docx\&gt;&quot;,

    &quot;filePath&quot;: &quot;Shared Documents/TermsAndConditions/\&lt;Application Name\&gt;&quot;,

    &quot;url&quot;:&quot;[https://altusgroup.sharepoint.com](https://altusgroup.sharepoint.com/)&quot;,

    &quot;site&quot;:&quot;TermsAndConditions&quot;,

    &quot;targetFolder&quot;:&quot;this/is/where/i/want/newfile&quot;

}

This endpoint is used to create a folder in sharepoint. Given a name and location of which to make the folder.

POST  https://{serverip}/api/flow/createFolderInSP

Headers

Content-Type application/json

x-flow-api-key : \&lt;the x-flow-api-key value\&gt;

Body

 {

    &quot;folderName&quot;: &quot;iwanttocreateafolderinthisone&quot;,

    &quot;folderPath&quot;: &quot;Shared Documents/\&lt;folder\&gt;&quot;,

    &quot;url&quot;: &quot;https://altusgroup.sharepoint.com&quot;,

    &quot;Site&quot;: &quot;TermsAndConditions&quot;

}

**Testing Services Portal**

Tests were made using the Jest testing framework.  There are both integration and unit tests for the Services Portal application.

**Integration Tests**

** **

We have completed jest integration tests on all of the endpoints in our services portal API. These tests are done for the purpose of showing that all of our endpoints run end to end and give expected output when compared with the input. To test that all the endpoints are running do npm test in the command line in the services portal directory and give a couple of minutes of runtime. Once run all tests should be passed this proves that everything is working as expected. If one or more of the tests fail then it is easy to diagnose the broken endpoint as well as often what the problem is within the endpoint.

**Unit Tests**

The only unit tests that have been created are for the setProjectNamesAndPermissions() function found in the file util/copy-to-s3-helper.js.  They include two important features that should be used in future unit tests.

Firstly, since the function being tested is private to its module (not exported), the rewire module is used to import the function.  Private functions can generally be imported with:

                const privateFunc = require(&#39;/path/to/module&#39;).\_\_get\_\_(&#39;privateFunc&#39;);

This is due to the use of babel-plugin-rewire, which is an application dependency.

Second, Jest and sequelize-mock were used to mock the database for the test suite.  The database mock definitions are found in the folder &quot;/\_\_tests\_\_/models/db/&quot;, although these definitions aren&#39;t currently being used in the actual tests.  Instead, each test adds mock objects to a queue, which are then output when a call to Sequelize (for example, findOne or findAll) is made.  See the mockAddProjects() and mockAddProjectContacts() functions for more details.

**Testing Altus SharePoint API**

No tests currently exist for Altus SharePoint API.  However, integration tests can easily be made, especially for the /api/flow/createFileFromString and /api/flow/copyToSp endpoints.  Both of these endpoints are very similar, as they call the same helper function which does most of the required work.  Also, they are both quite similar to the /api/main/copyFileS3ToSp endpoint of the Services Portal API, which has already been tested, so the integration tests for this endpoint should be very similar to those of the /api/main/copyFileS3ToSp endpoint of Services Portal.

**Other Important Notes**

**Using the APIs**

We have both APIs installed on the test server (10.71.235.133) as well as the production server (10.99.235.135), and both APIs are running on the production server.  The Services Portal API runs on port 3333 and Altus SharePoint API runs on port 4444, which are both open on the production server (but closed on the test server).

We also have domain names for the production instances of both APIs.  Firstly, portalq3.altusgrouplimited.com points to 10.99.235.135:3333 (Services Portal on production server), and portalq4.altusgrouplimited.com points to 10.99.235.135:4444 (Altus SharePoint API on production server).

The following is an outline for a standard call to an API endpoint:

[GET|PUT|POST] https://portalq[3|4].altusgrouplimited.com/api/[main|flow]/[endpointName]

Authorization: \&lt;AAD token\&gt;

\&lt;Other headers\&gt;

{

                &quot;bodykey1&quot;: &quot;bodyValue1&quot;,

                &quot;bodykey2&quot;: &quot;bodyValue2&quot;,

                ...

}

More details for a specific endpoint can be obtained by looking at the previous sections of this document, or by reading the README.md file or controllers/[flow|main].openid.json file within the API GitHub repository.

**What&#39;s using the APIs?**

Currently, several of the endpoints are being consumed by other applications.

1. **EstateMasterWebApp (Report Automation)**:  The Estate Master application was designed to allow users to easily create customized report documents.  This application makes a call to the /api/main/getTableauImage endpoint of the Services Portal API, in order to obtain an image of a view to be placed in a report.  In addition, the application calls the /api/flow/createFileFromString endpoint of the Altus SharePoint API in order to upload a JSON file to SharePoint, which in turn triggers a Microsoft Flow to create the final report.
2. **Document Generation** :  The Document Generation application makes a call to the /api/flow/createFileFromString endpoint of the Altus SharePoint API, in order to trigger a Microsoft Flow to generate the document.
3. **Microsoft Flow** :  The Flow entitled &quot;UpdateTermsAndConditions Flow&quot; is triggered when a Terms and Conditions docx document is dropped into a SharePoint folder.  The flow calls the /api/flow/updateTermsAndConditions endpoint of the Services Portal API, which converts the docx file into an html file and places the resulting html into a database table.
4. **Client Access Portal** :  The Client Access Portal indirectly uses the Services Portal /api/flow/updateTermsAndConditions endpoint.  When a Terms and Conditions document is uploaded to a SharePoint folder, the Flow described above is triggered, which places html inside a database table.  The Client Access Portal displays the latest Terms and Conditions document to have been placed in the SharePoint folder, and the user must agree to these Terms and Conditions before continuing into the Portal.

**API Configurations**

The APIs use environment variables and the &quot;dotenv&quot; package to store and load configurations.  A file called &quot;.env.example&quot; is a sample configuration file, so in order to create a configuration file from scratch, simply copy the contents of this file into a separate file named &quot;.env&quot; and add appropriate values for each variable listed.

There is a separate .env file to manage a testing environment, located in the &quot;\_\_tests\_\_&quot; directory.  This file is to contain the same environment variables as the main .env file, but with possibly different values.  Note that it is important to set the environment variable DB\_DATABASE to something different from the main database name, as this could cause data loss.

**Resources Interacted With**

Both APIs interact heavily with SharePoint. The Services Portal API has endpoints that interact with a MySQL database and Tableau.  The /api/flow/updateTermsAndConditions endpoint pulls a file from the TermsAndConditions SharePoint site, and the addition of a file to a specific folder within this SharePoint site triggers the Flow that calls this API endpoint.  There are also two Services Portal endpoints, /api/flow/copyFileSpToS3 and /api/flow/copyFileS3ToSp, which copy files between an S3 bucket and a SharePoint folder, where the SharePoint URL and site name are specified within the request bodies.  In addition to this, the Altus SharePoint API has endpoints which perform functionalities related to SharePoint, where the SharePoint URL and site name are specified within the request bodies.

MySQL is installed on both the test and production servers.  Most of the Services Portal endpoints interact with the altus\_cp MySQL database.  The altus\_cp\_test database is used for integration testing on the test server.  There are a few Services Portal endpoints that use the altus\_cp database:

●     The /api/main/isSuperAdmin endpoint searches the portal\_user table for a user with the specified email address and returns whether or not the user is a Super Admin.

●     The /api/main/copyFolderToS3, /api/main/copyFileToS3 and /api/main/copyFileSpToS3 endpoints copy files from SharePoint to an S3 bucket mount, and the destination directory within the bucket is determined by various database records.  The document table is queried to figure out the project ID associated with the document, which is then used to query the project and org\_business\_unit tables to find out the business unit and project name associated with the project.  The document is placed in the folder &quot;/DocumentManagement/\&lt;business unit\&gt;/\&lt;project ID\&gt;&quot; within the S3 bucket.

●     The /api/flow/updateTermsAndConditions endpoint first converts a SharePoint document to html.  If the document is the first one created for the specified application (which is specified by the final folder in the path of the new SharePoint document), a new record is inserted in the terms\_and\_conditions table, which contains the html of the document, and the version of the new document is set to 1.  Otherwise, the version is incremented of the existing record, and the html of the record is updated.  Finally, a record is inserted into the t\_and\_c\_versions table, which holds information about the new document version.

The Services Portal endpoint /api/main/getTableauImage uses a Tableau server to retrieve an image of a view.  The Tableau URL, site and view are passed in as body parameters.

Eventually, an admin account should be created for each resource that the APIs interact with (SharePoint, Tableau, possibly MySQL).  Since we store the credentials as environment variables, once this account is created, the .env files on the test and production servers would have to be updated to use the new credentials.  Note that the password of the admin account would have to be encrypted, as described below in the &quot;Encryption&quot; section.

As well, a SharePoint site designed for testing should probably be created in the future.  We currently use the folder _Shared Documents/Test_ for integration test documents, but it would probably be better to have a SharePoint site dedicated for this.

**Locking Functionality**

MySQL is used for several endpoints (_/api/flow/copyFileSpToS3_ and _/api/flow/updateTermsAndConditions_) in order to achieve a locking functionality.  This prevents request handlers from interfering with the resources being used by other request handlers.  The locks have keys of the form _\&lt;dbname\&gt;\_\&lt;key\&gt;_, where _\&lt;key\&gt;_ is the main part of the lock key.  We insert the database name at the beginning of the lock so that two databases won&#39;t interfere with each other&#39;s locks.  We also trim the end of the key if it is over 64 characters in length, as MySQL lock keys can&#39;t be more than 64 characters long.  Note that if a lock with a key is held by a session, it is impossible for another session to obtain that lock.

This locking functionality is used twice in the _/api/flow/updateTermsAndConditions_ endpoint.  Firstly, we have a lock with key _\&lt;dbname\&gt;\_\&lt;docname\&gt;_, where _\&lt;docname\&gt;_ is the name of the file from SharePoint.  This is so that a request handler can&#39;t overwrite the local copy of a SharePoint document of another request handler, which may happen if both request handlers are dealing with documents of the same name.  We also have a lock with key _\&lt;dbname\&gt;\_\&lt;appname\&gt;_, where _\&lt;appname\&gt;_ is the name of the application that the new Terms and Conditions document is associated with.  The reason for this is, there is a small chance that two request handlers dealing with documents of the same application can interfere with each other.  This may happen if, for example, both request handlers check that there is no record in the terms\_and\_conditions database table and proceed to insert a new record with version 1.  In this case, there will be two records in this table with the same application name, which breaks the primary key constraint.

A lock is also used in the /api/flow/copyFileSpToS3 endpoint, in order to prevent two documents of the same name from interfering with each other during the process of being pulled from SharePoint, as well as to prevent the metadata saving process from being corrupted.  The lock key for this endpoint is _\&lt;dbname\&gt;\_\&lt;filename\&gt;_, where _\&lt;filename\&gt;_ is the name of the file being pulled from SharePoint.

**Microsoft Flow Details**

There are two Flows that have been developed.  The first, entitled &quot;UpdateTermsAndConditions Flow&quot;, which is described briefly above, is triggered by a file being created in the &quot;/Shared Documents/TermsAndConditions/\&lt;application name\&gt;&quot; SharePoint folder on the TermsAndConditions SharePoint site.  We have created a custom connector in order to make a call to the /api/flow/updateTermsAndConditions endpoint of the Services Portal API, which converts the document to html and inserts it into the terms\_and\_conditions database table.  Finally, depending on the status code returned from the API call, an email is sent out with a success or failure message.  The failure messages are based on the JSON response body messages seen when the status code is not 200.

In addition, the second flow, entitled &quot;Report Automation Flow&quot;, is triggered when a user places a JSON file into a specific folder.   The file data is converted from an octet stream (which is the initial format of the data) to JSON format, which is then parsed by Flow.  There are three template documents that are permanently on SharePoint:  one for the report title page and intro, one for the table of contents, and one for the rest of the document.  The reasoning is, Plumsail has issues with tables of contents, but surprisingly all problems were fixed when the template was split into three docx documents and the middle template contained the table of contents.  Due to this workaround, the next step in the Flow is to use Plumsail to apply the JSON data to the first and third template, and to merge the resulting documents with the table of contents template document, again using Plumsail.  Finally, an email is sent to the recipients specified within the Flow, which has attached to it the generated document.

There are a few features of the Services Portal API that were forced by Microsoft Flow.  Firstly, the response bodies of _/api/flow_ endpoints have a property _actualStatusCode_, which holds the status code that the API intends to return.  Since Microsoft Flow fails when a non-200 status code is returned, the responses will always have a status code in the 200&#39;s, but _actualStatusCode_ is used within the Flow to determine the intended status of the request.

Secondly, the file controllers/flow.openid.json is an OpenAPI definition of the endpoints to be consumed by Flow.  Since Flow doesn&#39;t accept multiple request body parameters in OpenAPI definitions, we treat the entire body as one single body parameter within the OpenAPI definition, rather than as multiple body parameters.

**Running the Services Portal as a Daemon**

The command-line tool &quot;Forever&quot; has been installed on both the test server (10.71.235.133) and production server (10.99.235.135).  This allows Services Portal to run as a daemon on these servers.  To start and stop the daemon, navigate to the Services Portal directory and run the commands _forever start app.js_ and _stop app.js_.

**Encryption**

In order to protect passwords, the passwords in the environment variable files are encrypted using the encrypt() function found in the util/common.js file.  When used, they are first decrypted with the decrypt() function (in the same JavaScript file).  The encryption key can be found in the ENC\_KEY variable in the .env file.  If a set of credentials is updated, the encrypt() function must be used to obtain the new encrypted password.

**How to Create a Microsoft Custom Connector for the Services Portal API**

1. Navigate to[https://us.flow.microsoft.com/en-us/](https://us.flow.microsoft.com/en-us/).
2. Click on the gear icon on the top right of the page.
3. Select &quot;Custom Connectors&quot;.
4. Select &quot;New Custom Connector&quot;, then &quot;Import an OpenAPI file&quot;.
5. Name the new Custom Connector, select &quot;Import&quot;, and import the file &quot;services-portal/controllers/flow.openid.json&quot;.
6. (Optional) Select a connector icon and icon background colour.
7. Click the checkbox &quot;Connect via on-premises data gateway&quot;.
8. Ensure that you have an on premises data gateway and that it is running
9. Select the &quot;Create Connector&quot; button at the top of the screen.
10.

Note:  Avoid testing the custom connector with the &quot;Test&quot; tab, as it has been acting pretty buggy.

**How to Edit a Microsoft Custom Connector based on updated OpenAPI file**

1. Navigate to[https://us.flow.microsoft.com/en-us/](https://us.flow.microsoft.com/en-us/).
2. Click on the gear icon on the top right of the page.
3. Select &quot;Custom Connectors&quot;.
4. Select the &quot;...&quot; beside the connector you would like to update.
5. Select &quot;Update from OpenAPI file&quot;.
6. Click &quot;Import&quot; and select an OpenAPI file, then click &quot;Continue&quot;.

**How to Edit a Microsoft Flow**

1. Navigate to[https://us.flow.microsoft.com/en-us/](https://us.flow.microsoft.com/en-us/).
2. Select &quot;My Flows&quot;, then the &quot;My flows&quot; or &quot;Team flows&quot; tab, depending on where the Flow is located.
3. Hover over the Flow you would like to edit, and then select the pencil icon for &quot;Edit&quot;.

**Make API use HTTPS**

It is important for the APIs to use HTTPS because it adds an extra layer of security and it allows the APIs to be called from HTTPS-enabled websites.

In order to enable HTTPS, a self-signed private key, certificate and DH key must be generated with OpenSSL.

To create the private key and certificate:

sudo openssl req -x509 -nodes -days 365 -newkey rsa:2048 -keyout /path/to/private/key/selfsigned.key -out /path/to/cert/selfsigned.crt

Next, you must fill in information, such as your location and the company name.  Make sure to answer the prompt &quot;Common Name (e.g. server FQDN or YOUR name) []:&quot; with the server&#39;s IP address.

To create the DH key:

sudo openssl dhparam -out /path/to/dhparam/dhparam.crt 2048

To check the details of a generated certificate:

openssl x509 -text -noout -in /path/to/cert/selfsigned.crt

On the test and production servers, the private key is found at /etc/ssl/private/[test|prod]-selfsigned.key, and the certificate is found at /etc/ssl/certs/[test|prod]-selfsigned.crt, and the DH key is found at /etc/ssl/certs/[test|prod]-dhparam.crt.  In order for Node JS to be able to read the private key, which is by default in a directory owned by root, the following commands create a user group called &quot;certs&quot;, add the users bitnami and root to the group, and allow only group members to access the private key:

1. sudo groupadd certs
2. sudo usermod -a -G certs root
3. sudo usermod -a -G certs bitnami
4. At this point, log out and back into the server for these changes to take place.  You can verify that everything works with the command grep certs /etc/group, which should print out something similar to certs:x:\&lt;number\&gt;:bitnami,root.
5. sudo chgrp -R certs /etc/ssl/private
6. sudo chmod g\_x /etc/ssl/private
7. sudo chmod g\_x /etc/ssl

Finally, make sure to update the appropriate environment variables so that Node knows where to search for these SSL files.  It should look something like this, depending on how you named the certificate and key files:

SSL\_KEY\_PATH=&quot;/etc/ssl/private/[test|prod]-selfsigned.key&quot;

SSL\_CERT\_PATH=&quot;/etc/ssl/certs/[test|prod]-selfsigned.crt&quot;

DH\_KEY\_PATH=&quot;/etc/ssl/certs/[test|prod]-dhparam.crt&quot;
